---
title: "Triangulating iconicity: Coding analysis"
author: "Mark Dingemanse, Stella Punselie, Bonnie McLean"
date: "Updated `r format(Sys.Date())`"
output: github_document
editor_options: 
  chunk_output_type: console
---
  
  ```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=6, fig.path='figures_md/',
                      echo=TRUE, warning=FALSE, message=FALSE)

```

Code notebook for a study of the relation between linguistically informed iconicity coding and experimentally collected guessability scores.

## Setup

```{r preliminaries, results="hide"}
# Packages
list.of.packages <- c("tidyverse","readxl","writexl","ggthemes","gghalves","ggbeeswarm","viridis","lme4","VGAM","cowplot")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages, require, character.only=T)

# useful functions
`%notin%` <- function(x,y) !(x %in% y) 
mean.na <- function(x) mean(x, na.rm = T)
sd.na <- function(x) sd(x, na.rm = T)

# custom functions
source("TI_functions.R")
```

## Data
Here we load the ground truth version of the coded data and add the independently collected guessability scores from the 2016 Collabra and Language papers.

```{r data}

# get consensus coding data
d = read_excel("data/ideophones_coded.xlsx") %>% arrange(filename)

# add guessability scores from the Collabra and Language studies.

d.scores = read_excel("data/ideophones_guessability.xlsx") %>%
  dplyr::select(-category)
d <- left_join(d,d.scores,by=c("ideophone","language","study" = "paper"))

# add logodds (for when we run stats: it's more sensible to predict against logodds than raw proportion correct)
d <- d %>%
  group_by(study) %>%
  mutate(logodds = probitlink(score))

# add Z score to make scores more comparable in plots across studies
d <- d %>%
  group_by(study) %>%
  mutate(score_z = scale(score,center=T,scale=T))

# get ratings data
d.ratings <- read_xlsx("data/ideophones_rated_means.xlsx") %>%
  dplyr::select(-category,-list,-item)
d <- left_join(d,d.ratings,by=c("filename","study","language")) |>
  mutate(filename = gsub("_org","",filename))

# add z score for ratings
d$rating_z <- scale(d$rating,center=T,scale=T)

# write this dataset as the fullest dataset
write.csv(d,file="data/ideophones_coded_guessed_rated.csv",fileEncoding = "UTF-8")
write_xlsx(d,path="data/ideophones_coded_guessed_rated.xlsx")

# also write a simplified copy for easy inspection in Excel (helpful when writing)
d %>%
  dplyr::select(ideophone,meaning,language,category,meaning_NL,study,score,rating,C_aspect,C_magnitude,C_modality,C_length,C_irregular,C_cumulative) %>%
  write_xlsx(path="data/ideophones_coded_guessed_rated_simplified.xlsx")

ideophones_filename <- d |> ungroup() |> select(ideophone,filename) |>
  mutate(filename = str_to_lower(filename))

# load the full ratings data for some of the reporting in the paper
d.ratings.full <- read_xlsx("data\\ideophones_rated.xlsx") |>
  mutate(list = gsub(".*([A-Z])$", "\\1",item)) |>
  mutate(filename = gsub("^(.*)_.*$", "\\1", str_to_lower(item))) |>
  mutate(filename = gsub("Coll_","", filename)) |>
  select(-item) 

# match the data to ideophones for easier lookup
d.ratings.full <- left_join(d.ratings.full,
                  ideophones_filename,by="filename",copy=TRUE)


```

## Figures
Here we generate some descriptive stats and create the main figures for the paper.

```{r descriptive_stats}

d %>%
  group_by(study) %>%
  dplyr::summarize(max=max(score),min=min(score),mean=mean.na(score))

```

### Figure: Overview panel
The overview panel is created separately but we use the data from the guessability studies as a canvas.

```{r figures_paper_guessability_canvas}

# make a copy dp of the data in which we convert some measures to factors for easy plotting & facetting
tofactors <- paste(c("language|category|study|group",names(d[grep('C_',names(d))])),collapse = "|")
dp <- d
dp[,grep(tofactors,names(dp))] <- lapply(dp[,grep(tofactors,names(dp))], as.factor)


# blank plot can be useful as a canvas 
ggplot(data=dp, aes(x=study,y=score)) +
  theme_tufte() + ylim(0,1) + theme(legend.position="none",axis.title.x=element_blank(),plot.margin=margin(0,0,10,0)) + 
  stat_summary(fun=median,geom="point",size=8,shape=21,stroke=1,fill="white",colour="#c9c9c9") +
  geom_dotplot(colour="white",fill="black",stackgroups=T,dotsize=1.5,binwidth=0.01,binaxis="y",stackdir = "center")

ggsave("figures\\fig2-guessability-blank.pdf",height=5,width=7.5)


```

Let's plot the guessability by study and iconicity rating

```{r figures_paper_guessability_rating}

# panel A: scores by study and cumulative iconicity
pA <- ggplot(data=dp, aes(x=study,y=score,fill=study,colour=study)) +
  theme_tufte() + ylim(0,1) + theme(legend.position="none") + 
  labs(title="Guessability by study",
       x="study",
       y="guessability (% correct)") +
  geom_dotplot(stackgroups=T,dotsize=1.5,binwidth=0.01,binaxis="y",stackdir = "center") +
  NULL

pB <- ggplot(data=dp, aes(x=rating,y=score,fill=study,colour=study)) +
  theme_tufte() + ylim(0,1) + theme(legend.position="none") +
  labs(title="Guessability (both studies) by iconicity rating",
       x="iconicity rating",
       y="") +
  geom_point(size=2.0,shape=21) +
  geom_smooth(method="lm") +
  NULL

plot_grid(pA,pB,labels=c("A","B"),label_size=14,rel_widths = c(1.4,2))
#ggsave("figures\\paper_panelAB_guessing_rating.png",height=5,width=9)



```

### Figure 3: Guessability and cumulative iconicity

```{r figures_paper_guessability_coding}

# panel A: scores by study and cumulative iconicity
pA <- ggplot(data=dp, aes(x=study,y=score,fill=C_cumulative,colour=C_cumulative)) +
  theme_tufte() + ylim(0,1) + theme(legend.position="none") + 
  labs(title="Guessability by study",
       x="study",
       y="guessability (% correct)") +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  stat_summary(fun.y=median,geom="point",size=8,shape=21,stroke=1,fill="white") +
  geom_dotplot(stackgroups=T,dotsize=1.5,binwidth=0.01,binaxis="y",stackdir = "center") +
  NULL

pB <- ggplot(data=dp, aes(x=C_cumulative,y=score_z,colour=C_cumulative)) +
  theme_tufte() + theme(legend.position="none") +
  labs(title="Guessability (both studies) by cumulative iconicity",
       x="cumulative iconicity",
       y=expression('guessability ('~italic(z)~')')) +
  geom_half_boxplot() +
  geom_half_point() +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  NULL

plot_grid(pA,pB,labels=c("A","B"),label_size=14,rel_widths = c(1.4,2))
ggsave("figures\\fig3-panelAB.png",height=4,width=9,bg="white")


```

### Figure 4: Ratings by study (A) and by semantic domain (B)

Panel figure for Study B providing a view of ratings per study and per semantic domain.

```{r fig4_panel}


#panel A: ratings by study
pA <- d |>  ggplot(aes(x=score_z,y=rating)) +   
  theme_tufte(base_size = 16) +
  theme(legend.position = c(0.2,0.9),
        legend.title = element_blank()) +
  scale_fill_manual(values=c("white","black")) +
  geom_point(aes(fill=study),
             position="jitter",
             shape=21,
             colour="black") +
  geom_smooth(method=loess,colour="black",alpha=0.5) +
  xlab("guessability (z)") + ylab("rating") +
  NULL

#panel B: ratings by semantic domain
pB <- d |>
  filter(category != "Other") |>
  ggplot(aes(x=reorder(category,-rating),y=rating,color=category)) +
  theme_tufte(base_size=16) +
  geom_half_boxplot(aes(middle=mean(rating)),show.legend=F) +
  geom_half_point(show.legend=F) +
  scale_colour_viridis_d(option="D",alpha=0.8) +
  xlab("category") + scale_x_discrete(labels = c("Sound","Motion","Shape", 
  "Texture","Colour/Visual"))

plot_grid(pA,pB,labels=c("A","B"),label_size=14,rel_widths = c(1.6,2))
ggsave("figures/fig4-panel_ratings.png",height=4,width=9,bg="white")


# a correlation test shows a Pearson correlation of .57 between iconicity rating
# and guessability
cor.test(d$rating_z,d$logodds,method="pearson")


```

Table 3: Examples of ideophones from the low/high rating/guessability quadrants

```{r example_ratings_guessables}

# these examples are given in Table 3 in the paper
examples <- c("ton ton","kpa",
              "slʔẽẽk","miɔmiɔ",
              "kodzokodzo","ɟtoonɟtoonɟtoon",
              "fututu", "plɒ̃s")
d |>
  filter(ideophone %in% examples) |>
  group_by(ideophone,language) |>
  dplyr::summarise(n=n(),
                   mean_rating = mean(rating),
                   score = score)


```


### Figure 5: Ratings and cumulative iconicity

```{r ratings_and_C}


# panel A: ratings by study and cumulative iconicity
pA <- ggplot(data=dp, aes(x="",y=rating,fill=C_cumulative,colour=C_cumulative)) +
  theme_tufte() +  ylim(1,5) + theme(legend.position="none") + 
  labs(title="Iconicity ratings for 239 ideophones",
       x="all ideophones",
       y="mean iconicity rating") +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  geom_beeswarm(cex=2.5) +
  NULL

pB <- ggplot(data=dp, aes(x=C_cumulative,y=rating,colour=C_cumulative)) +
  theme_tufte() +  ylim(1,5) + theme(legend.position="none") +
  labs(title="... by independently coded structural correspondences",
       x="cumulative iconicity",
       y="mean iconicity rating") +
  geom_half_boxplot() +
  geom_half_point() +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  NULL

plot_grid(pA,pB,labels=c("A","B"),label_size=14,rel_widths = c(1.4,2))
ggsave("figures/fig6-panelAB_ratings.png",height=5,width=9,bg="white")


```

## Numbers

### Coding and guessability
As the visualizations already make clear there is a non-trivial positive relation between the presence of iconic mappings (in a cumulative sense) and the guessability score determined in experimental work.

The simplest thing to do is a correlation. 

```{r congruency_stats_1}

# simplest thing we can do is a correlation. 

cor.test(d$logodds,d$C_cumulative,method="pearson")

d.Collabra <- d %>% filter(study == "Collabra")
d.Language <- d %>% filter(study == "Language")
cor.test(d.Collabra$logodds,d.Collabra$C_cumulative,method="pearson")
cor.test(d.Language$logodds,d.Language$C_cumulative,method="pearson")

p0 <- cor.test(d$logodds,d$C_cumulative,method="pearson")$p.value
p1 <- cor.test(d.Collabra$logodds,d.Collabra$C_cumulative,method="pearson")$p.value
p2 <- cor.test(d.Language$logodds,d.Language$C_cumulative,method="pearson")$p.value
p.adjust(c(p0,p1,p2),method="bonferroni")
pvalues <-p.adjust(c(p0,p1,p2),method="bonferroni")  

```

But it's probably more useful to predict logodds score on the basis of what we know about paper (= a proxy for method) and the main measures of form-meaning congruence. For this we can use logistic regression.

A null model `m0` predices logodds by paper, controlling for item-level differences. A model `m1` with cumulative congruence as an additional fixed effect fares significantly better.

For the fixed effects, m1 lists a 0.182 improvement in logodds for C_cumulative. According to my interpretation this means that every added iconic feature leads to a ~7% boost in proportion correct.

Also note that, accounting for possible difference in coded cumulative iconicity across studies, the Language paper has a negative fixed effect of -0.524, which translates to a 19% guessability penalty for the relatively unforgiving method of the Language study (or alternatively, a similar-sized boost for the methods of the Collabra study).

```{r congruency_stats_2}


m0 <- lmer(logodds ~ study + (1|ideophone), data=d)
m1 <- lmer(logodds ~ study + C_cumulative + (1|ideophone), data=d)
anova(m0,m1)

# fixed effects
fixef(m1)

# turn this into a percentage under the simplifying assumption that the distance
# between iconicity measures is the same at every scale point
probitlink(fixef(m1)[[3]],inverse=T)-0.5
probitlink(fixef(m1)[[2]],inverse=T)-0.5



```

We can assess the relative weight of different features using model comparison. We build models by adding the congruency measures in rank order of attestation in the coded data. Doing this we find that C_aspect, C_modality and C_length are responsible for the bulk of the variance. Compared to the null model that only represents the baseline difference between studies, model `m8` fares significantly better (log likelihood difference = 21.3).

The model estimates given in the output of `fixef(m8)` represent the contribution of a given fixed effect while controlling for others, but they are NOT additive (i.e. can't be summed to arrive at the effect of `C_cumulative`).

* `C_aspect`: 0.181 logodds (+7% boost)
* `C_modality`: 0.177 logodds (+7% boost)
* `C_length`: 0.443 logodds (+17% boost)

```{r congruency_stats_3}

m3 <- lmer(logodds ~ study + C_aspect + (1|ideophone), data=d)
m4 <- lmer(logodds ~ study + C_aspect + C_magnitude + (1|ideophone), data=d)
m5 <- lmer(logodds ~ study + C_aspect + C_magnitude + C_modality + (1|ideophone), data=d)
m6 <- lmer(logodds ~ study + C_aspect + C_magnitude + C_modality + C_length + (1|ideophone), data=d)
m7 <- lmer(logodds ~ study + C_aspect + C_magnitude + C_modality + C_length + C_irregular + (1|ideophone), data=d)
m8 <- lmer(logodds ~ study + C_aspect + C_modality + C_length + (1|ideophone), data=d)

anova(m0,m3) # aspect helps
anova(m3,m4) # magnitude doesn't additionally help
anova(m4,m5) # modality helps
anova(m3,m5) # modality helps also relative to m3
anova(m5,m6) # length helps
anova(m6,m7) # irregular doesn't additionally help
anova(m5,m8) # aspect + modality + length is where it's at
# so m8 is the best performing most complex model
anova(m0,m8)
summary(m8)
fixef(m8)

probitlink(fixef(m8)[1],inverse=T)-0.5
probitlink(fixef(m8)[2],inverse=T)-0.5
probitlink(fixef(m8)[3],inverse=T)-0.5 # aspect
probitlink(fixef(m8)[4],inverse=T)-0.5 # modality
probitlink(fixef(m8)[5],inverse=T)-0.5 # length



```

### Structural correspondences and semantic domains 

Our prediction is that Sound and Motion ideophones should show more structural correspondences than the other domains. This is borne out.

```{r descriptive_semantic_domains}

dp.categories <- d %>%
  filter(category %notin% "Other") %>%
  mutate(category = forcats::fct_reorder(category, -C_cumulative, .fun='mean')) %>%
  mutate(category_binary = ifelse(category %in% c("Sound","Motion"),"SoundMotion","ColorvisualShapeTexture")) %>%
  mutate(C_nosound = C_aspect + C_length + C_irregular + C_magnitude) %>% # for control analysis
  mutate(C_strongestpredictors = C_aspect + C_length + C_modality ) # for another control analysis


# mean of C_cumuluative
dp.categories |>
  group_by(category) |>
  dplyr::summarise(n=n(),mean_C=mean.na(C_cumulative),mean_rating=mean.na(rating),mean_score=mean.na(score)) %>%
  arrange(-mean_C)

# mean of only the strongest predictors
dp.categories |>
  group_by(category) |>
  dplyr::summarise(n=n(),mean_strongest=mean.na(C_strongestpredictors),mean_rating=mean.na(rating),mean_score=mean.na(score)) |>
  arrange(-mean_strongest)

# binary comparison
dp.categories |>
  ungroup() |>
  group_by(category_binary) %>%
  dplyr::summarise(n=n(),mean_C=mean.na(C_cumulative),mean_rating=mean.na(rating),mean_score=mean.na(score)) |>
  arrange(-mean_C)

group1 <- subset(dp.categories, category_binary == "SoundMotion")$C_cumulative
group2 <- subset(dp.categories, category_binary == "ColorvisualShapeTexture")$C_cumulative
t.test(group1,group2)
effsize::cohen.d(group1,group2)

# also when we exclude C_modality? yes
dp.categories |>
  group_by(category) |>
  dplyr::summarise(n=n(),mean_C=mean.na(C_nosound),mean_rating=mean.na(rating),mean_score=mean.na(score)) |>
  arrange(-mean_C)
dp.categories |>
  group_by(category_binary) |>
  dplyr::summarise(n=n(),mean_C=mean.na(C_nosound),mean_rating=mean.na(rating),mean_score=mean.na(score)) |>
  arrange(-mean_C)

group1 <- subset(dp.categories, category_binary == "SoundMotion")$C_nosound
group2 <- subset(dp.categories, category_binary == "ColorvisualShapeTexture")$C_nosound
t.test(group1,group2)
effsize::cohen.d(group1,group2)

```


### Study B: Rating iconicity

Here we look at the iconicity ratings for all ideophones across both studies.

```{r coding_ratings_descriptives}

d.ratings.full |>
  group_by(ideophone,language) |>
  dplyr::summarise(n=n(),mean=mean.na(rating),sd=sd.na(rating))

# get means and standard deviations across all ideophones and for each language and category
mean(d.allratings$rating)
  # 2.948339
sd(d.allratings$rating)

examples <- c("kpa","boo boo","zuratto", "plɒ̃s")
d.ratings.full |>
  filter(ideophone %in% examples) |>
  group_by(ideophone,language) |>
  dplyr::summarise(n=n(),
                   mean_rating = mean(rating),
                   sd_rating = sd(rating))


```

There appears to be a positive correlation between the rating and guessing data.


### Ratings: numbers

Iconicity ratings by semantic domain

```{r ratings_by_domain}

d.ratings.full |>
  filter(category != "Other") |>
  group_by(category) |>
  dplyr::summarise(n=n(),
                   mean_rating=mean(rating),
                   sd_rating = sd(rating)) |>
  arrange(desc(mean_rating))


```



```{r linear_regressions}

```


How do iconicity ratings relate to structural correspondences?

A simple correlation test suggests a positive relation between iconicity rating and cumulative iconicity. As above, we can go a bit deeper by using mixed effects modelling. We expect a smaller effect of study on the ratings, as the difference in design in terms of selection of foils falls away.

```{r coding_ratings_lme}

# simple correlation test
cor.test(d$rating_z,d$C_cumulative,method="pearson")

# lme
m0 <- lmer(rating ~ study + (1|ideophone), data=d)
m1 <- lmer(rating ~ study + C_cumulative + (1|ideophone), data=d)
anova(m0,m1)

fixef(m1)

```

As above, we can assess the relative weight of different features using model comparison. We build models by adding the congruency measures in rank order of attestation in the coded data. Doing this we find that C_aspect, C_modality and C_length are responsible for the bulk of the variance. Compared to the null model that only represents the baseline difference between studies, model `m8` fares significantly better (log likelihood difference = 21.3).

The model estimates given in the output of `fixef(m8)` represent the contribution of a given fixed effect while controlling for others, but they are NOT additive (i.e. can't be summed to arrive at the effect of `C_cumulative`).

* `C_aspect`: 0.181 logodds (+7% boost)
* `C_modality`: 0.177 logodds (+7% boost)
* `C_length`: 0.443 logodds (+17% boost)

```{r congruency_stats_3}

m3 <- lmer(rating ~ study + C_aspect + (1|ideophone), data=d)
m4 <- lmer(rating ~ study + C_aspect + C_magnitude + (1|ideophone), data=d)
m5 <- lmer(rating ~ study + C_aspect + C_magnitude + C_modality + (1|ideophone), data=d)
m6 <- lmer(rating ~ study + C_aspect + C_magnitude + C_modality + C_length + (1|ideophone), data=d)
m7 <- lmer(rating ~ study + C_aspect + C_magnitude + C_modality + C_length + C_irregular + (1|ideophone), data=d)
m8 <- lmer(rating ~ study + C_aspect + C_modality + C_length + (1|ideophone), data=d)

anova(m0,m3) # aspect helps
anova(m3,m4) # magnitude doesn't additionally help
anova(m4,m5) # modality helps
anova(m3,m5) # modality helps also relative to m3
anova(m5,m6) # length helps
anova(m6,m7) # irregular doesn't additionally help
anova(m5,m8) # aspect + modality + length is where it's at
# so m8 is the best performing most complex model
anova(m0,m8)
summary(m8)
fixef(m8)



```
