---
title: "Triangulating iconicity: Coding analysis"
author: "Mark Dingemanse, Stella Punselie, Bonnie McLean"
date: "Updated `r format(Sys.Date())`"
output: github_document
editor_options: 
  chunk_output_type: console
---
  
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=6, fig.path='figures_md/',
                      echo=TRUE, warning=FALSE, message=FALSE)

```

Code notebook for a study of the relation between linguistically informed iconicity coding and experimentally collected guessability scores.

## Setup

```{r preliminaries, results="hide"}
# Packages
list.of.packages <- c("tidyverse","ggthemes","gghalves","ggbeeswarm","viridis","lme4","VGAM","cowplot")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages, require, character.only=T)

# useful functions
`%notin%` <- function(x,y) !(x %in% y) 
mean.na <- function(x) mean(x, na.rm = T)
sd.na <- function(x) sd(x, na.rm = T)

```

## Data
Here we load the data. For earlier processing operations, see `paper_00_data_processing.R`.

```{r load_data}
d <- readr::read_csv("data/ideophones_coded_guessed_rated.csv")

d.ratings.full <- readr::read_csv("data/ideophones_rated.csv")

```

## Figures
Here we generate some descriptive stats and create the main figures for the paper. Note that Figures 1 and 2 in the paper are illustrative and their final versions are not generated in R.

```{r descriptive_stats}

d |>
  group_by(study) |>
  dplyr::summarize(max=max(score),min=min(score),mean=mean.na(score))

```

### Figure 2: Overview panel
The overview of the triangulation method is created separately in CorelDraw but we use the data from the guessability studies as a canvas.

```{r figures_paper_guessability_canvas}

# make a copy dp of the data in which we convert some measures to factors for easy plotting & faceting
tofactors <- paste(c("language|category|study|group",names(d[grep('C_',names(d))])),collapse = "|")
dp <- d
dp[,grep(tofactors,names(dp))] <- lapply(dp[,grep(tofactors,names(dp))], as.factor)

# blank plot can be useful as a canvas 
ggplot(data=dp, aes(x=study,y=score)) +
  theme_tufte() + ylim(0,1) + theme(legend.position="none",axis.title.x=element_blank(),plot.margin=margin(0,0,10,0)) + 
  stat_summary(fun=median,geom="point",size=8,shape=21,stroke=1,fill="white",colour="#c9c9c9") +
  geom_dotplot(colour="white",fill="black",stackgroups=T,dotsize=1.5,binwidth=0.01,binaxis="y",stackdir = "center")

ggsave("figures/fig2-guessability-blank.pdf",height=5,width=7.5)


```

### Figure 3: Guessability and cumulative iconicity

```{r figures_paper_guessability_coding}

# panel A: scores by study and cumulative iconicity
pA <- ggplot(data=dp, aes(x=study,y=score,fill=C_cumulative,colour=C_cumulative)) +
  theme_tufte() + ylim(0,1) + theme(legend.position="none") + 
  labs(title="Guessability by study",
       x="study",
       y="guessability (% correct)") +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  stat_summary(fun.y=median,geom="point",size=8,shape=21,stroke=1,fill="white") +
  geom_dotplot(stackgroups=T,dotsize=1.5,binwidth=0.01,binaxis="y",stackdir = "center") +
  NULL

pB <- ggplot(data=dp, aes(x=C_cumulative,y=score_z,colour=C_cumulative)) +
  theme_tufte() + theme(legend.position="none") +
  labs(title="Guessability (both studies) by cumulative iconicity",
       x="cumulative iconicity",
       y=expression('guessability ('~italic(z)~')')) +
  geom_half_boxplot() +
  geom_half_point() +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  NULL

plot_grid(pA,pB,labels=c("A","B"),label_size=14,rel_widths = c(1.4,2))
ggsave("figures/fig3-panelAB.png",height=4,width=9,bg="white")


```

### Figure 4: Ratings by study (A) and by semantic domain (B)

Panel figure for Study B providing a view of ratings per study and per semantic domain.

```{r fig4_panel}

#panel A: ratings by study
pA <- d |>  ggplot(aes(x=score_z,y=rating)) +   
  theme_tufte(base_size = 16) +
  theme(legend.position = c(0.2,0.9),
        legend.title = element_blank()) +
  scale_fill_manual(values=c("white","black")) +
  geom_point(aes(fill=study),
             position="jitter",
             shape=21,
             colour="black") +
  geom_smooth(method=loess,colour="black",alpha=0.5) +
  xlab("guessability (z)") + ylab("rating") +
  NULL

#panel B: ratings by semantic domain
pB <- d |>
  filter(category != "Other") |>
  ggplot(aes(x=reorder(category,-rating),y=rating,color=category)) +
  theme_tufte(base_size=16) +
  geom_half_boxplot(aes(middle=mean(rating)),show.legend=F) +
  geom_half_point(show.legend=F) +
  scale_colour_viridis_d(option="D",alpha=0.8) +
  xlab("category") + scale_x_discrete(labels = c("Sound","Motion","Shape", 
  "Texture","Colour/Visual"))

plot_grid(pA,pB,labels=c("A","B"),label_size=14,rel_widths = c(1.6,2))
ggsave("figures/fig4-panel_ratings.png",height=4,width=9,bg="white")


# a correlation test shows a Pearson correlation of .57 between iconicity rating
# and guessability
cor.test(d$rating_z,d$logodds,method="pearson")


```

Table 3: Examples of ideophones from the low/high rating/guessability quadrants

```{r example_ratings_guessables}

# these examples are given in Table 3 in the paper
examples <- c("ton ton","kpa",
              "slʔẽẽk","miɔmiɔ",
              "kodzokodzo","ɟtoonɟtoonɟtoon",
              "fututu", "plɒ̃s")
d |>
  filter(ideophone %in% examples) |>
  group_by(ideophone,language) |>
  dplyr::summarise(n=n(),
                   mean_rating = mean(rating),
                   score = score)


```


### Figure 5: Ratings and cumulative iconicity

```{r ratings_and_C}


# panel A: ratings by study and cumulative iconicity
pA <- ggplot(data=dp, aes(x="",y=rating,fill=C_cumulative,colour=C_cumulative)) +
  theme_tufte() +  ylim(1,5) + theme(legend.position="none") + 
  labs(title="Iconicity ratings for 239 ideophones",
       x="all ideophones",
       y="mean iconicity rating") +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  geom_beeswarm(cex=2.5) +
  NULL

pB <- ggplot(data=dp, aes(x=C_cumulative,y=rating,colour=C_cumulative)) +
  theme_tufte() +  ylim(1,5) + theme(legend.position="none") +
  labs(title="... by independently coded structural correspondences",
       x="cumulative iconicity",
       y="mean iconicity rating") +
  geom_half_boxplot() +
  geom_half_point() +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  NULL

plot_grid(pA,pB,labels=c("A","B"),label_size=14,rel_widths = c(1.4,2))
ggsave("figures/fig6-panelAB_ratings.png",height=5,width=9,bg="white")


```

## Numbers

### Coding and guessability
As the visualizations already make clear there is a non-trivial positive relation between the presence of iconic mappings (in a cumulative sense) and the guessability score determined in experimental work.

The simplest thing to do is a correlation. 

```{r congruency_stats_1}

# Pearson correlation for both of the studies
d.Collabra <- d |> filter(study == "Collabra")
d.Language <- d |> filter(study == "Language")
correlation.Collabra <- cor.test(d.Collabra$logodds,d.Collabra$C_cumulative,method="pearson")
correlation.Language <- cor.test(d.Language$logodds,d.Language$C_cumulative,method="pearson")

# Bonferroni-correcting the p values for multiple comparisons
p1 <- correlation.Collabra$p.value
p2 <- correlation.Language$p.value
pvalues <-p.adjust(c(p1,p2),method="bonferroni")  

```

The correlation is found in the Collabra study (Pearson’s r = `r round(correlation.Collabra$estimate,2)`, p = `r pvalues[1]`) as well as in the Language study (Pearson’s r = `r round(correlation.Language$estimate,2)`, p `r pvalues[2]`, p-values Bonferroni-corrected for multiple comparisons).

But it's probably more useful to predict logodds score on the basis of what we know about paper (= a proxy for method) and the main measures of form-meaning congruence. For this we can use logistic regression.

A null model `m0` predices logodds by paper, controlling for item-level differences. A model `m1` with cumulative congruence as an additional fixed effect fares significantly better.

For the fixed effects, m1 lists a 0.182 improvement in logodds for C_cumulative. According to my interpretation this means that every added iconic feature leads to a ~7% boost in proportion correct.

Also note that, accounting for possible difference in coded cumulative iconicity across studies, the Language paper has a negative fixed effect of -0.524, which translates to a 19% guessability penalty for the relatively unforgiving method of the Language study (or alternatively, a similar-sized boost for the methods of the Collabra study).

```{r congruency_stats_2}


m0 <- lmer(logodds ~ study + (1|ideophone), data=d)
m1 <- lmer(logodds ~ study + C_cumulative + (1|ideophone), data=d)
anova(m0,m1)

# fixed effects
fixef(m1)

# turn this into a percentage under the simplifying assumption that the distance
# between iconicity measures is the same at every scale point
probitlink(fixef(m1)[[3]],inverse=T)-0.5
probitlink(fixef(m1)[[2]],inverse=T)-0.5



```

We can assess the relative weight of different features using model comparison. We build models by adding the congruency measures in rank order of attestation in the coded data. Doing this we find that C_aspect, C_modality and C_length are responsible for the bulk of the variance. Compared to the null model that only represents the baseline difference between studies, model `m8` fares significantly better (log likelihood difference = 21.3).

The model estimates given in the output of `fixef(m8)` represent the contribution of a given fixed effect while controlling for others, but they are NOT additive (i.e. can't be summed to arrive at the effect of `C_cumulative`).

* `C_aspect`: 0.181 logodds (+7% boost)
* `C_modality`: 0.177 logodds (+7% boost)
* `C_length`: 0.443 logodds (+17% boost)

```{r congruency_stats_3}

m3 <- lmer(logodds ~ study + C_aspect + (1|ideophone), data=d)
m4 <- lmer(logodds ~ study + C_aspect + C_magnitude + (1|ideophone), data=d)
m5 <- lmer(logodds ~ study + C_aspect + C_magnitude + C_modality + (1|ideophone), data=d)
m6 <- lmer(logodds ~ study + C_aspect + C_magnitude + C_modality + C_length + (1|ideophone), data=d)
m7 <- lmer(logodds ~ study + C_aspect + C_magnitude + C_modality + C_length + C_irregular + (1|ideophone), data=d)
m8 <- lmer(logodds ~ study + C_aspect + C_modality + C_length + (1|ideophone), data=d)

anova(m0,m3) # aspect helps
anova(m3,m4) # magnitude doesn't additionally help
anova(m4,m5) # modality helps
anova(m3,m5) # modality helps also relative to m3
anova(m5,m6) # length helps
anova(m6,m7) # irregular doesn't additionally help
anova(m5,m8) # aspect + modality + length is where it's at
# so m8 is the best performing most complex model
anova(m0,m8)
summary(m8)
fixef(m8)

probitlink(fixef(m8)[1],inverse=T)-0.5
probitlink(fixef(m8)[2],inverse=T)-0.5
probitlink(fixef(m8)[3],inverse=T)-0.5 # aspect
probitlink(fixef(m8)[4],inverse=T)-0.5 # modality
probitlink(fixef(m8)[5],inverse=T)-0.5 # length



```

### Structural correspondences and semantic domains 

Our prediction is that Sound and Motion ideophones should show more structural correspondences than the other domains. This is borne out.

```{r descriptive_semantic_domains}

dp.categories <- d |>
  filter(category %notin% "Other") |>
  mutate(category = forcats::fct_reorder(category, -C_cumulative, .fun='mean')) |>
  mutate(category_binary = ifelse(category %in% c("Sound","Motion"),"SoundMotion","ColorvisualShapeTexture")) |>
  mutate(C_nosound = C_aspect + C_length + C_irregular + C_magnitude) |> # for control analysis
  mutate(C_strongestpredictors = C_aspect + C_length + C_modality ) # for another control analysis


# mean of C_cumuluative
dp.categories |>
  group_by(category) |>
  dplyr::summarise(n=n(),mean_C=mean.na(C_cumulative),mean_rating=mean.na(rating),mean_score=mean.na(score)) |>
  arrange(-mean_C)

# mean of only the strongest predictors
dp.categories |>
  group_by(category) |>
  dplyr::summarise(n=n(),mean_strongest=mean.na(C_strongestpredictors),mean_rating=mean.na(rating),mean_score=mean.na(score)) |>
  arrange(-mean_strongest)

# binary comparison
dp.categories |>
  ungroup() |>
  group_by(category_binary) |>
  dplyr::summarise(n=n(),mean_C=mean.na(C_cumulative),mean_rating=mean.na(rating),mean_score=mean.na(score)) |>
  arrange(-mean_C)

group1 <- subset(dp.categories, category_binary == "SoundMotion")$C_cumulative
group2 <- subset(dp.categories, category_binary == "ColorvisualShapeTexture")$C_cumulative
t.test(group1,group2)
effsize::cohen.d(group1,group2)

# also when we exclude C_modality? yes
dp.categories |>
  group_by(category) |>
  dplyr::summarise(n=n(),mean_C=mean.na(C_nosound),mean_rating=mean.na(rating),mean_score=mean.na(score)) |>
  arrange(-mean_C)
dp.categories |>
  group_by(category_binary) |>
  dplyr::summarise(n=n(),mean_C=mean.na(C_nosound),mean_rating=mean.na(rating),mean_score=mean.na(score)) |>
  arrange(-mean_C)

group1 <- subset(dp.categories, category_binary == "SoundMotion")$C_nosound
group2 <- subset(dp.categories, category_binary == "ColorvisualShapeTexture")$C_nosound
t.test(group1,group2)
effsize::cohen.d(group1,group2)

```


### Study B: Rating iconicity

Here we look at the iconicity ratings for all ideophones across both studies.

```{r coding_ratings_descriptives}

d.ratings.full |>
  group_by(ideophone,language) |>
  dplyr::summarise(n=n(),mean=mean.na(rating),sd=sd.na(rating))

# get means and standard deviations across all ideophones and for each language and category
mean(d.ratings.full$rating)

sd(d.ratings.full$rating)

examples <- c("kpa","boo boo","zuratto", "plɒ̃s")
d.ratings.full |>
  filter(ideophone %in% examples) |>
  group_by(ideophone,language) |>
  dplyr::summarise(n=n(),
                   mean_rating = mean(rating),
                   sd_rating = sd(rating))


```

There appears to be a positive correlation between the rating and guessing data.


### Ratings: numbers

Iconicity ratings by semantic domain

```{r ratings_by_domain}

d.ratings.full |>
  filter(category != "Other") |>
  group_by(category) |>
  dplyr::summarise(n=n(),
                   mean_rating=mean(rating),
                   sd_rating = sd(rating)) |>
  arrange(desc(mean_rating))


```


How do iconicity ratings relate to structural correspondences?

```{r coding_ratings_correlation}
correlation_coding_ratings <- cor.test(d$rating_z,d$C_cumulative,method="pearson")
```

A simple correlation test suggests a positive relation between iconicity rating and cumulative iconicity: Pearson's *r* = `r round(correlation_coding_ratings$estimate,2)`, p = `r correlation_coding_ratings$p.value`. 

As above, we can go a bit deeper by using mixed effects modelling. We expect a smaller effect of study on the ratings, as the difference in design in terms of selection of foils falls away.

```{r coding_ratings_lme}

# lme
m0 <- lmer(rating ~ study + (1|ideophone), data=d)
m1 <- lmer(rating ~ study + C_cumulative + (1|ideophone), data=d)
anova(m0,m1)

m0m1.chisquare <- round(anova(m0,m1)$Chisq[2],2)
m0m1.pvalue <- anova(m0,m1)$"Pr(>Chisq)"[2]
m0m1.loglikdiff <- abs(anova(m0,m1)$logLik[1] - anova(m0,m1)$logLik[2])

fixef(m1)

```

We find that a model with cumulative iconicity as an additional fixed effect achieves a fit that is significantly better than a null model without it (χ2(1) = `r m0m1.chisquare`, p = `r m0m1.pvalue`, log likelihood difference `r m0m1.loglikdiff`). The fixed effect of cumulative iconicity is estimated at `r fixef(m1)[["C_cumulative"]]`. 

```{r ratings_stats_3}

m3 <- lmer(rating ~ study + C_aspect + (1|ideophone), data=d)
m4 <- lmer(rating ~ study + C_aspect + C_magnitude + (1|ideophone), data=d)
m5 <- lmer(rating ~ study + C_aspect + C_magnitude + C_modality + (1|ideophone), data=d)
m6 <- lmer(rating ~ study + C_aspect + C_magnitude + C_modality + C_length + (1|ideophone), data=d)
m7 <- lmer(rating ~ study + C_aspect + C_magnitude + C_modality + C_length + C_irregular + (1|ideophone), data=d)
m8 <- lmer(rating ~ study + C_aspect + C_modality + C_length + (1|ideophone), data=d)

anova(m0,m3) # aspect helps
anova(m3,m4) # magnitude doesn't additionally help
anova(m4,m5) # modality helps
anova(m3,m5) # modality helps also relative to m3
anova(m5,m6) # length helps
anova(m6,m7) # irregular doesn't additionally help
anova(m5,m8) # aspect + modality + length is where it's at
# so m8 is the best performing most complex model
anova(m0,m8)

m0m8.chisquare <- round(anova(m0,m8)$Chisq[2],2)
m0m8.pvalue <- anova(m0,m8)$"Pr(>Chisq)"[2]
m0m8.loglikdiff <- abs(anova(m0,m8)$logLik[1] - anova(m0,m8)$logLik[2])

m8.estimates <- summary(m8)[["coefficients"]][,1]

fixef(m8)



```

We can also assess the relative weight of different features using model comparison. We build models by adding the congruency measures in rank order of attestation in the coded data. Doing this we find that C_aspect, C_modality and C_length are responsible for the bulk of the variance. Compared to the null model that only represents the baseline difference between studies, model `m8` fares significantly better (Chisquare = `r m0m8.chisquare`, p = `r m0m8.pvalue`, log likelihood difference = `r m0m8.loglikdiff`).


The model estimates given in the output of `fixef(m8)` represent the contribution of a given fixed effect while controlling for others, but they are NOT additive (i.e. they cannot be summed to arrive at the effect of `C_cumulative`).

* `C_length`: `r round(m8.estimates["C_length"],2)`
* `C_modality`: `r round(m8.estimates["C_modality"],2)`
* `C_aspect`: `r round(m8.estimates["C_aspect"],2)`

