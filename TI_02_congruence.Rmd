---
title: "Triangulating iconicity: From structure mapping to guessability"
author: "Mark Dingemanse & Stella Punselie"
date: "Updated `r format(Sys.Date())`"
output: github_document
editor_options: 
  chunk_output_type: console
---
  
  ```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=6, fig.path='figures_md/',
                      echo=TRUE, warning=FALSE, message=FALSE)

```

Code notebook for a study of the relation between linguistically informed iconicity coding and experimentally collected guessability scores.

## Setup

```{r preliminaries, results="hide"}
# Packages
list.of.packages <- c("tidyverse","readxl","writexl","ggthemes","gghalves","ggbeeswarm","viridis","lme4","VGAM","cowplot")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages, require, character.only=T)

# useful functions
`%notin%` <- function(x,y) !(x %in% y) 
mean.na <- function(x) mean(x, na.rm = T)
sd.na <- function(x) sd(x, na.rm = T)

# custom functions
source("TI_functions.R")
```

## Data
Here we load the ground truth version of the coded data and add the independently collected guessability scores from the 2016 Collabra and Language papers.

```{r data}

# get coding data
d = read_excel("data\\ideophones_coded.xlsx") %>% arrange(filename)

coding_categories <- d %>% dplyr::select(matches('_'),-matches('notes|meaning')) %>% names()

# add guessability scores from the Collabra and Language studies.

d.scores = read_excel("data\\ideophones_guessability.xlsx") %>%
  dplyr::select(-category)
d <- left_join(d,d.scores,by=c("ideophone","language","study" = "paper"))

# add logodds (for when we run stats: it's more sensible to predict against logodds than raw proportion correct)
d <- d %>%
  group_by(study) %>%
  mutate(logodds = probitlink(score))

# add Z score to account for differences in base success rate of Collabra and Language Studies
d <- d %>%
  group_by(study) %>%
  mutate(score_z = scale(score,center=T,scale=T))

```

We add congruency measures that link F and M elements. 

* TO DISCUSS: We need to think about the evidential value of each of the congruency measures. Most apply to many words, which is good. If some of them only capture a few (as `C_durative` and `C_weight_tone` seem to do), they are not so interesting because they don't really capture recurring iconic mappings in the data. 
* Discussed: This made us realize a need to name `M_durative` to `M_long`, covering extent not just in time but also in space (a very concrete kind of mapping for `F_finallength`. 

```{r congruency_1}

# using ifelse() statements that essentially say, if these conditions are true, use 1, otherwise 0
d <- d %>%
  mutate(C_modality = ifelse(M_sound == 1,1,0),
         C_iterative = ifelse(F_redup == 1 & M_distribution == 1,1,0),
         C_irregular = ifelse(F_redupmod == 1 & M_irregular == 1,1,0),
         C_closure = ifelse(F_closedsyllable == 1 & M_abrupt == 1,1,0),
         C_punctual = ifelse(F_monosyllabic == 1 & M_punctual == 1,1,0),
         C_long = ifelse(F_finallength == 1 & M_long == 1,1,0),
         C_weight_voice = ifelse((F_voice == 0 & M_weight == 0) | (F_voice == 2 & M_weight == 2),1,0),
         C_weight_vowel = ifelse((F_vowelquality == 0 & M_weight == 0) | (F_vowelquality == 2 & M_weight == 2),1,0),
         C_weight_tone = ifelse((F_intonation == 0 & M_weight == 2) | (F_intonation == 2 & M_weight == 0),1,0)
         )

# some mappings seem to be mutually exclusive, so it might be fitting to combine them
d %>%
  mutate(C_aspect = C_iterative + C_punctual) %>%
  dplyr::select(ideophone,meaning_NL,language,C_iterative,C_punctual,C_event) %>%
  filter(C_aspect > 1)

d %>%
  mutate(C_length = C_long + C_closure) %>%
  dplyr::select(ideophone,meaning_NL,language,C_iterative,C_closure,C_length) %>%
  filter(C_length > 1)

d <- d %>%
  mutate(C_aspect = C_iterative + C_punctual,
         C_length = C_long + C_closure)


# C_weight is recombined as all three form subfeatures are linked to one and the same meaning feature M_weight

d %>% # sanity check
  mutate(C_weight = C_weight_voice + C_weight_vowel + C_weight_tone) %>%
  dplyr::select(ideophone,meaning_NL,language,C_weight_voice,C_weight_vowel,C_weight_tone,C_weight) %>%
  filter(C_weight > 0)

d <- d %>%
  mutate(C_weight = C_weight_voice + C_weight_vowel + C_weight_tone,
         C_magnitude = ifelse(C_weight == 0,0,1)) # i.e. a magnitude value that is 1 if any weight feature was congruent


# add cumulative iconicity scores
d <- d %>%
  mutate(C_cumulative = C_modality + C_aspect + C_length + C_irregular + C_magnitude) %>%
  mutate(C_any1 = ifelse(C_cumulative == 0,0,1),
         C_atleast2 = ifelse(C_cumulative > 1,1,0),
         C_ternary = ifelse(C_cumulative == 0,0,
                            ifelse(C_cumulative == 1,1,
                                   ifelse(C_cumulative == 2,2,3)))) # i.e. 1, 2 or 3


# add a simplified cumulative score by summing only the 3 correspondences that account for more than 10% of attested congruent mappings

d <- d %>%
  mutate(C_simple = C_modality + C_aspect + C_magnitude)

# write this dataset for use in the next step
write.csv(d,file="data/ideophones_coded_guessed.csv")

# also write a simplified copy for easy inspection in Excel (helpful when writing)
d %>%
  dplyr::select(ideophone,meaning,language,category,meaning_NL,study,score,C_aspect,C_magnitude,C_modality,C_length,C_irregular,C_cumulative) %>%
  write_xlsx(path="data/ideophones_coded_guessed_simplified.xlsx")
  

d %>%
  group_by(C_cumulative) %>%
  summarise(n=n(),score=mean.na(score))

d %>%
  group_by(C_any1) %>%
  summarise(n=n(),score=mean.na(score))

d %>%
  group_by(C_atleast2) %>%
  summarise(n=n(),score=mean.na(score))

d %>%
  group_by(C_ternary) %>%
  summarise(n=n(),score=mean.na(score))

d %>%
  group_by(C_simple) %>%
  summarise(n=n(),score=mean.na(score))


```

### Figures
To visualise the results we create the figures for the paper here.

```{r figures_paper}

# make a copy dp of the data in which we convert some measures to factors for easy plotting & facetting
tofactors <- paste(c("language|category|study|group",names(d[grep('C_',names(d))])),collapse = "|")
dp <- d
dp[,grep(tofactors,names(dp))] <- lapply(dp[,grep(tofactors,names(dp))], as.factor)


# panel A: scores by study and cumulative iconicity
pA <- ggplot(data=dp, aes(x=study,y=score,fill=C_cumulative,colour=C_cumulative)) +
  theme_tufte() + ylim(0,1) + theme(legend.position="none") + 
  labs(title="Guessability by study",
       x="study",
       y="guessability (% correct)") +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  stat_summary(fun.y=median,geom="point",size=8,shape=21,stroke=1,fill="white") +
  geom_dotplot(stackgroups=T,dotsize=1.5,binwidth=0.01,binaxis="y",stackdir = "center")

pB <- ggplot(data=dp, aes(x=C_cumulative,y=score_z,colour=C_cumulative)) +
  theme_tufte() + theme(legend.position="none") +
  labs(title="Guessability (both studies) by cumulative iconicity",
       x="cumulative iconicity",
       y=expression('guessability ('~italic(z)~')')) +
  geom_half_boxplot() +
  geom_half_point() +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  NULL

plot_grid(pA,pB,labels=c("A","B"),label_size=14,rel_widths = c(1.4,2))
ggsave("figures\\paper_panelAB.png",height=5,width=9)


```


### Raincloud plots
While the icoplots provide a useful first visualisation, for in a paper we would probably want to use something like raincloud plots that show key features of the distribution across groups.

```{r visuals}

# make a copy dp of the data in which we convert some measures to factors for easy plotting & facetting
tofactors <- paste(c("language|category|study|group",names(d[grep('C_',names(d))])),collapse = "|")
dp <- d
dp[,grep(tofactors,names(dp))] <- lapply(dp[,grep(tofactors,names(dp))], as.factor)


# panel A: scores by study and cumulative iconicity
ggplot(data=dp, aes(x=study,y=score,fill=C_cumulative,colour=C_cumulative)) +
  theme_tufte() + ylim(0,1) + theme(legend.position="none",axis.title.x=element_blank()) + 
  labs(title="Guessability by study and cumulative iconicity") +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  stat_summary(fun.y=median,geom="point",size=8,shape=21,stroke=1,fill="white") +
  geom_dotplot(stackgroups=T,dotsize=1.5,binwidth=0.01,binaxis="y",stackdir = "center")


ggplot(data=dp, aes(x=C_cumulative,y=logodds)) +
  theme_tufte() +
  labs(title="Guessability (log odds) by cumulative iconicity",
       x="cumulative iconicity",
       y="guessability (log odds)") +
  geom_half_boxplot() +
  geom_half_point() +
  NULL
ggsave("figures\\coding_guessability_by_cumulative.png",height=5,width=7.5)

ggplot(data=dp, aes(x=C_any1,y=logodds)) +
  theme_tufte() +
  geom_half_boxplot() +
  geom_half_point() +
  NULL


ggplot(data=dp, aes(x=C_atleast2,y=logodds)) +
  theme_tufte() +
  geom_half_boxplot() +
  geom_half_point() +
  NULL

ggplot(data=dp, aes(x=C_ternary,y=logodds)) +
  theme_tufte() +
  geom_half_boxplot() +
  geom_half_point() +
  NULL


ggplot(data=dp, aes(x="all",y=score_z)) +
  theme_tufte() +
  geom_half_boxplot() +
  geom_half_point(side="l") +
  geom_half_violin(side="r") +
  NULL

ggplot(data=dp, aes(x=C_cumulative,y=logodds,color=C_cumulative,fill=C_cumulative)) +
  theme_tufte() +
  geom_half_violin() +
  geom_half_point_panel() +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) 

icoplot("C_cumulative")

ggplot(data=dp, aes(x=C_cumulative,y=logodds,color=C_cumulative,fill=C_cumulative)) +
  theme_tufte() +
  geom_half_violin() +
  geom_half_point_panel() +
  scale_fill_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  scale_colour_viridis(option="plasma",discrete=T,begin=0.3,end=0.9) +
  facet_grid(~ study)

```


## Dotplots

A quick look at some congruency measures. Uses a custom plotting function `icoplot()`, which is a wrapper for a ggplot object constructed with `geom_dotplot`.


```{r icoplots}


# all data points by language
icoplot() # icoplot() is loaded from TI_functions.R

icoplot("C_modality")
icoplot("C_aspect")
icoplot("C_length")
icoplot("C_magnitude")
icoplot("C_irregular")
icoplot("C_cumulative")


icoplot("C_any1")
icoplot("C_atleast2")
icoplot("C_ternary")
icoplot("C_simple")

# blank plot can be useful as background for slides
ggplot(data=dp, aes(x=study,y=score)) +
  theme_tufte() + ylim(0,1) + theme(legend.position="none",axis.title.x=element_blank(),plot.margin=margin(0,0,10,0)) + 
  stat_summary(fun.y=median,geom="point",size=8,shape=21,stroke=1,fill="white",colour="#c9c9c9") +
  geom_dotplot(colour="white",fill="#c9c9c9",stackgroups=T,dotsize=1.5,binwidth=0.01,binaxis="y",stackdir = "center")


d %>%
  plot_ly(x = ~logodds, 
          y = ~C_cumulative,
          type="scatter3d",
          mode="markers",
          color= ~rating_z,
          name=~ideophone,
          showlegend=F) %>%
  layout(showlegend=F)


ggsave("figures\\paper-guessability-blank.eps",height=5,width=7.5)

```


## Statistical tests
As the visualizations already make clear there is a non-trivial positive relation between the presence of iconic mappings (in a cumulative sense) and the guessability score determined in experimental work.

The simplest thing to do is a simple correlation. Looks like there is an r=0.30 correlation between logodds and C_cumulative.

```{r congruency_stats_1}

# simplest thing we can do is a correlation. 

cor.test(d$logodds,d$C_cumulative,method="pearson")

d.Collabra <- d %>% filter(study == "Collabra")
d.Language <- d %>% filter(study == "Language")
cor.test(d.Collabra$logodds,d.Collabra$C_cumulative,method="pearson")
cor.test(d.Language$logodds,d.Language$C_cumulative,method="pearson")

p0 <- cor.test(d$logodds,d$C_cumulative,method="pearson")$p.value
p1 <- cor.test(d.Collabra$logodds,d.Collabra$C_cumulative,method="pearson")$p.value
p2 <- cor.test(d.Language$logodds,d.Language$C_cumulative,method="pearson")$p.value
p.adjust(c(p0,p1,p2),method="bonferroni")


```

But it's probably more useful to predict logodds score on the basis of what we know about paper (= a proxy for method) and the main measures of form-meaning congruence. For this we can use logistic regression.

A null model `m0` predices logodds by paper, controlling for item-level differences. A model `m1` with cumulative congruence as an additional fixed effect fares significantly better.

For the fixed effects, m1 lists a 0.182 improvement in logodds for C_cumulative. According to my interpretation this means that every added iconic feature leads to a ~7% boost in proportion correct.

Also note that, accounting for possible difference in coded cumulative iconicity across studies, the Language paper has a negative fixed effect of -0.524, which translates to a 19% guessability penalty for the relatively unforgiving method of the Language study (or alternatively, a similar-sized boost for the methods of the Collabra study).

```{r congruency_stats_2}


m0 <- lmer(logodds ~ study + (1|ideophone), data=d)
m1 <- lmer(logodds ~ study + C_cumulative + (1|ideophone), data=d)
anova(m0,m1)

fixef(m1)

probitlink(0.1823,inverse=T)-0.5
probitlink(-0.5242,inverse=T)-0.5



```

We can assess the relative weight of different features using model comparison. We build models by adding the congruency measures in rank order of attestation in the coded data. Doing this we find that C_aspect, C_modality and C_length are responsible for the bulk of the variance. Compared to the null model that only represents the baseline difference between studies, model `m8` fares significantly better (log likelihood difference = 21.3).

The model estimates given in the output of `fixef(m8)` represent the contribution of a given fixed effect while controlling for others, but they are NOT additive (i.e. can't be summed to arrive at the effect of `C_cumulative`).

* `C_aspect`: 0.181 logodds (+7% boost)
* `C_modality`: 0.177 logodds (+7% boost)
* `C_length`: 0.443 logodds (+17% boost)

```{r congruency_stats_3}

m3 <- lmer(logodds ~ study + C_aspect + (1|ideophone), data=d)
m4 <- lmer(logodds ~ study + C_aspect + C_magnitude + (1|ideophone), data=d)
m5 <- lmer(logodds ~ study + C_aspect + C_magnitude + C_modality + (1|ideophone), data=d)
m6 <- lmer(logodds ~ study + C_aspect + C_magnitude + C_modality + C_length + (1|ideophone), data=d)
m7 <- lmer(logodds ~ study + C_aspect + C_magnitude + C_modality + C_length + C_irregular + (1|ideophone), data=d)
m8 <- lmer(logodds ~ study + C_aspect + C_modality + C_length + (1|ideophone), data=d)

anova(m0,m3) # aspect helps
anova(m3,m4) # magnitude doesn't additionally help
anova(m4,m5) # modality helps
anova(m3,m5) # modality helps also relative to m3
anova(m5,m6) # length helps
anova(m6,m7) # irregular doesn't additionally help
anova(m5,m8) # aspect + modality + length is where it's at
# so m8 is the best performing most complex model
anova(m0,m8)
summary(m8)
fixef(m8)

probitlink(fixef(m8)[1],inverse=T)-0.5
probitlink(fixef(m8)[2],inverse=T)-0.5
probitlink(fixef(m8)[3],inverse=T)-0.5
probitlink(fixef(m8)[4],inverse=T)-0.5
probitlink(fixef(m8)[5],inverse=T)-0.5

```


## Exploring congruency blind spots
What are we not capturing? Our coding and congruency measures are designed to be reproducible and human-codable. They are not exhaustive, and there may well be aspects that we are missing. Some ideophones may also be guessed relatively high for reasons other than form-meaning analogies (e.g., they had a very unattractive foil or presented unforeseen confounds). Here we explore the congruency data to find highly guessed ideophones that our coding did not (yet) identify as having iconic mappings. We also look at the other end: ideophones coded as having an iconic mapping but not doing well in the binary forced choice task. 

If we sort by score most of the blindspots are from the Collabra study, which had a less forbidding design and therefore a higher baseline score. 

```{r congruency_blindspots}


d %>%
  filter(C_cumulative == 0, score_z > 0.5) %>%
  dplyr::select(filename,ideophone,meaning,meaning_NL,language,logodds,score) %>%
  arrange(-score) %>% ungroup %>%
  slice(1:10)

```


```{r congruency_underperformers}
# the reverse is also interesting: which ideophones were coded as having iconic mappings yet were not guessed greatly?

d %>%
  filter(C_cumulative > 0, logodds < -0.25) %>%
  dplyr::select(filename,ideophone,meaning,meaning_NL,C_cumulative,language,logodds,score) %>%
  arrange(score) %>% ungroup() %>%
  slice(1:10)


```
